# Model Cache

Loading a large language model into memory (especially onto GPU) is expensive. The model cache keeps already-loaded `transformers.pipeline` objects in memory so that consecutive `run_task()` calls using the same model skip the loading step entirely.

The cache is **optional**. When `model_cache=None` (the default), every `run_task()` call loads the model from scratch.

## Architecture

The cache follows a **Protocol-based** design with a single method `load(key, model_loader)`:

- `ModelCache` (`src/gpt_task/cache/abc.py`) — the Protocol interface, generic over the cached object type. Any object with a matching `load` method can be used as a cache, no inheritance required.
- `MemoryModelCache` (`src/gpt_task/cache/memory_impl.py`) — the built-in implementation.

## Cache Key

Cache key is generated by `generate_model_key()` in `src/gpt_task/inference/key.py`. It is an MD5 hash derived from three fields of `GPTTaskArgs`:

- `model` — model name / HuggingFace repo id
- `dtype` — torch dtype (`float16`, `bfloat16`, `float32`, `auto`)
- `quantize_bits` — optional 4-bit or 8-bit quantization

Two calls with the same model name but different dtypes or quantization will produce different keys and occupy separate cache slots.

## MemoryModelCache Behavior

`MemoryModelCache` stores pipelines in a `dict` with a configurable `max_size` (default `1`).

- **Cache hit** — if the key exists in the dict, return the stored pipeline immediately.
- **Cache miss** — call the `model_loader` callable to load the model, store the result, then return it.
- **Eviction** — when the cache is full (`len >= max_size`), the first inserted entry is evicted (FIFO). After eviction, `torch.cuda.empty_cache()` is called to free GPU memory if CUDA is available.

## Integration in `run_task()`

`run_task()` in `src/gpt_task/inference/inference.py` accepts an optional `model_cache` parameter. When provided:

1. A cache key is generated from the task args.
2. A `model_loader` closure is defined that builds the full `transformers.pipeline` (including dtype, quantization, tokenizer, etc.).
3. `model_cache.load(key, model_loader)` is called — the loader is only invoked on a cache miss.

When `model_cache` is `None`, the loader is called directly every time.

The prompt rendering step in `run_task()` executes after the pipeline is loaded. The cache contract remains unchanged because cache keys continue to depend only on `model`, `dtype`, and `quantize_bits`.

## Usage

See `examples/qwen3_with_cache_example.py` for a complete cache example. The key pattern is: create a single `MemoryModelCache` instance and pass it to every `run_task()` call. The first call loads the model; subsequent calls reuse the cached pipeline.
